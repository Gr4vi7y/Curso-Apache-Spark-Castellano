{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplos con Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción de las variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset, obtenido de <a target = \"_blank\" href=\"https://www.transtats.bts.gov/Fields.asp?Table_ID=236\">este link</a> está compuesto por las siguientes variables referidas siempre al año 2018:\n",
    "\n",
    "1. **Month** 1-4\n",
    "2. **DayofMonth** 1-31\n",
    "3. **DayOfWeek** 1 (Monday) - 7 (Sunday)\n",
    "4. **FlightDate** fecha del vuelo\n",
    "5. **Origin** código IATA del aeropuerto de origen\n",
    "6. **OriginCity** ciudad donde está el aeropuerto de origen\n",
    "7. **Dest** código IATA del aeropuerto de destino\n",
    "8. **DestCity** ciudad donde está el aeropuerto de destino  \n",
    "9. **DepTime** hora real de salida (local, hhmm)\n",
    "10. **DepDelay** retraso a la salida, en minutos\n",
    "11. **ArrTime** hora real de llegada (local, hhmm)\n",
    "12. **ArrDelay** retraso a la llegada, en minutos: se considera que un vuelo ha llegado \"on time\" si aterrizó menos de 15 minutos más tarde de la hora prevista en el Computerized Reservations Systems (CRS).\n",
    "13. **Cancelled** si el vuelo fue cancelado (1 = sí, 0 = no)\n",
    "14. **CancellationCode** razón de cancelación (A = aparato, B = tiempo atmosférico, C = NAS, D = seguridad)\n",
    "15. **Diverted** si el vuelo ha sido desviado (1 = sí, 0 = no)\n",
    "16. **ActualElapsedTime** tiempo real invertido en el vuelo\n",
    "17. **AirTime** en minutos\n",
    "18. **Distance** en millas\n",
    "19. **CarrierDelay** en minutos: El retraso del transportista está bajo el control del transportista aéreo. Ejemplos de sucesos que pueden determinar el retraso del transportista son: limpieza de la aeronave, daño de la aeronave, espera de la llegada de los pasajeros o la tripulación de conexión, equipaje, impacto de un pájaro, carga de equipaje, servicio de comidas, computadora, equipo del transportista, problemas legales de la tripulación (descanso del piloto o acompañante) , daños por mercancías peligrosas, inspección de ingeniería, abastecimiento de combustible, pasajeros discapacitados, tripulación retrasada, servicio de inodoros, mantenimiento, ventas excesivas, servicio de agua potable, denegación de viaje a pasajeros en mal estado, proceso de embarque muy lento, equipaje de mano no válido, retrasos de peso y equilibrio.\n",
    "20. **WeatherDelay** en minutos: causado por condiciones atmosféricas extremas o peligrosas, previstas o que se han manifestado antes del despegue, durante el viaje, o a la llegada.\n",
    "21. **NASDelay** en minutos: retraso causado por el National Airspace System (NAS) por motivos como condiciones meteorológicas (perjudiciales pero no extremas), operaciones del aeropuerto, mucho tráfico aéreo, problemas con los controladores aéreos, etc.\n",
    "22. **SecurityDelay** en minutos: causado por la evacuación de una terminal, re-embarque de un avión debido a brechas en la seguridad, fallos en dispositivos del control de seguridad, colas demasiado largas en el control de seguridad, etc.\n",
    "23. **LateAircraftDelay** en minutos: debido al propio retraso del avión al llegar, problemas para conseguir aterrizar en un aeropuerto a una hora más tardía de la que estaba prevista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos el fichero CSV utilizando el delimitador por defecto de Spark (\",\"). La primera línea contiene encabezados (nombres de columnas) por lo que no es parte de los datos y debemos indicarlo con la opción header."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Leemos los datos desde el CSV que hay en Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto no hace nada: la lectura es lazy así que no se lee en realidad hasta que ejecutemos una acción sobre flightsDF\n",
    "# Solamente se comprueba que exista el fichero en esa ruta, y se leen los nombres de columnas\n",
    "flightsDF = spark.read.option(\"header\", \"true\")\\\n",
    "                 .csv(\"gs://data/flights-jan-apr-2018.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vamos a mostrar el contenido del metastore de Hive, que ahora mismo no tiene nada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creamos una vista temporal de un DF que tiene solo dos columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto solamente añade metadatos al metastore de Hive, que además se borrarán cuando cerremos el notebook. No guarda datos físicos de la tabla en ningún lado, puesto que el DF está en memoria. O mejor dicho, el DF del que proviene esta vista temporal \"no está en ningún lado\", porque no hemos cacheado weatherDistanceDF así que cualquier consulta SQL que hagamos sobre la tabla `weatherDistanceTable` provoca que se tenga que re-calcular el DF `weatherDistanceDF` sobre el cual está creada dicha tabla temporal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherDistanceDF = flightsDF.select(\"WeatherDelay\", \"Distance\")\n",
    "weatherDistanceDF.createOrReplaceTempView(\"weatherDistanceTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vemos que la tabla se ha creado como tabla temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "|        |weatherdistancetable|       true|\n",
      "+--------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos hacer consultas sobre ella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|WeatherDelay|Distance|\n",
      "+------------+--------+\n",
      "|        null|  374.00|\n",
      "|        null|  198.00|\n",
      "|        0.00|  198.00|\n",
      "|        null|  198.00|\n",
      "|        null|  198.00|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from weatherDistanceTable limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creamos una tabla persistente manejada, guardando como tabla el resultado de una operación con el DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsJFK = flightsDF.where(\"Origin = 'JFK'\")\n",
    "flightsJFK.write.saveAsTable(\"flightsjfk\") # es una acción: se guardan físicamente los datos en algún sitio de HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si volvemos a mostrar las tablas que existen, veremos la nueva. Vemos que **no** es temporal, pero no sabemos si es manejada o es externa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "| default|          flightsjfk|      false|\n",
      "|        |weatherdistancetable|       true|\n",
      "+--------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para saber si una tabla es temporal, o bien es externa o es manejada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|               Month|              string|   null|\n",
      "|          DayofMonth|              string|   null|\n",
      "|           DayOfWeek|              string|   null|\n",
      "|          FlightDate|              string|   null|\n",
      "|              Origin|              string|   null|\n",
      "|          OriginCity|              string|   null|\n",
      "|                Dest|              string|   null|\n",
      "|            DestCity|              string|   null|\n",
      "|             DepTime|              string|   null|\n",
      "|            DepDelay|              string|   null|\n",
      "|             ArrTime|              string|   null|\n",
      "|            ArrDelay|              string|   null|\n",
      "|           Cancelled|              string|   null|\n",
      "|    CancellationCode|              string|   null|\n",
      "|            Diverted|              string|   null|\n",
      "|   ActualElapsedTime|              string|   null|\n",
      "|             AirTime|              string|   null|\n",
      "|            Distance|              string|   null|\n",
      "|        CarrierDelay|              string|   null|\n",
      "|        WeatherDelay|              string|   null|\n",
      "|            NASDelay|              string|   null|\n",
      "|       SecurityDelay|              string|   null|\n",
      "|   LateAircraftDelay|              string|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|            Database|             default|       |\n",
      "|               Table|          flightsjfk|       |\n",
      "|               Owner|                root|       |\n",
      "|        Created Time|Fri Jun 12 19:37:...|       |\n",
      "|         Last Access|Thu Jan 01 00:00:...|       |\n",
      "|          Created By|         Spark 2.3.4|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|    Table Properties|[transient_lastDd...|       |\n",
      "|          Statistics|        494667 bytes|       |\n",
      "|            Location|hdfs://ucmcluster...|       |\n",
      "|       Serde Library|org.apache.hadoop...|       |\n",
      "|         InputFormat|org.apache.hadoop...|       |\n",
      "|        OutputFormat|org.apache.hadoop...|       |\n",
      "|  Storage Properties|[serialization.fo...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe formatted flightsjfk\").show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|    Type|  MANAGED|       |\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe formatted flightsjfk\").where(\"col_name = 'Type'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Guardamos flightsDF como fichero parquet en HDFS (nada de tablas aún)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsSFO = flightsDF.where(\"Origin = 'SFO'\")\\\n",
    "                      .select(\"FlightDate\", \"Origin\", \"Dest\", \"Distance\")\n",
    "flightsSFO.write.mode(\"overwrite\").parquet(\"/flightsSFO.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "-rw-r--r--   2 root hadoop          0 2020-06-27 10:58 /flightsSFO.parquet/_SUCCESS\n",
      "-rw-r--r--   2 root hadoop      23854 2020-06-27 10:58 /flightsSFO.parquet/part-00000-b35f850f-3f08-414a-b167-e6d68e7870a3-c000.snappy.parquet\n",
      "-rw-r--r--   2 root hadoop      29340 2020-06-27 10:58 /flightsSFO.parquet/part-00001-b35f850f-3f08-414a-b167-e6d68e7870a3-c000.snappy.parquet\n",
      "-rw-r--r--   2 root hadoop      13899 2020-06-27 10:58 /flightsSFO.parquet/part-00002-b35f850f-3f08-414a-b167-e6d68e7870a3-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /flightsSFO.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ahora vamos a crear una tabla EXTERNA a partir del fichero /flightsSFO.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ojo: tenemos que especificar bien el esquema de la tabla que estamos creando a partir del fichero. El DF que hemos guardado en Parquet tenía 4 columnas, todas de tipo string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FlightDate: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsSFO.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create external table flightsSFO(FlightDate string, Origin string, Dest string, Distance string)\\\n",
    "          stored as parquet location '/flightsSFO.parquet'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprobamos que ahora tenemos una tabla más, persistente, llamada flightSFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|database|tableName           |isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "|default |flightsjfk          |false      |\n",
      "|default |flightssfo          |false      |\n",
      "|        |weatherdistancetable|true       |\n",
      "+--------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿La tabla flightsSFO es externa, o es manejada?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|          FlightDate|              string|   null|\n",
      "|              Origin|              string|   null|\n",
      "|                Dest|              string|   null|\n",
      "|            Distance|              string|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|            Database|             default|       |\n",
      "|               Table|          flightssfo|       |\n",
      "|               Owner|                root|       |\n",
      "|        Created Time|Sat Jun 27 11:08:...|       |\n",
      "|         Last Access|Thu Jan 01 00:00:...|       |\n",
      "|          Created By|         Spark 2.3.4|       |\n",
      "|                Type|            EXTERNAL|       |\n",
      "|            Provider|                hive|       |\n",
      "|    Table Properties|[transient_lastDd...|       |\n",
      "|          Statistics|         67093 bytes|       |\n",
      "|            Location|hdfs://ucmcluster...|       |\n",
      "|       Serde Library|org.apache.hadoop...|       |\n",
      "|         InputFormat|org.apache.hadoop...|       |\n",
      "|        OutputFormat|org.apache.hadoop...|       |\n",
      "|  Storage Properties|[serialization.fo...|       |\n",
      "|  Partition Provider|             Catalog|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe formatted flightssfo\").show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|    Type| EXTERNAL|       |\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe formatted flightssfo\").where(\"col_name = 'Type'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Borramos tabla externa y comprobamos que el fichero Parquet sigue ahí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "| default|          flightsjfk|      false|\n",
      "|        |weatherdistancetable|       true|\n",
      "+--------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"drop table flightssfo\")\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprobemos que sigue existiendo la carpeta /flightsSFO.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "-rw-r--r--   2 root hadoop          0 2020-06-27 10:58 /flightsSFO.parquet/_SUCCESS\n",
      "-rw-r--r--   2 root hadoop      23854 2020-06-27 10:58 /flightsSFO.parquet/part-00000-b35f850f-3f08-414a-b167-e6d68e7870a3-c000.snappy.parquet\n",
      "-rw-r--r--   2 root hadoop      29340 2020-06-27 10:58 /flightsSFO.parquet/part-00001-b35f850f-3f08-414a-b167-e6d68e7870a3-c000.snappy.parquet\n",
      "-rw-r--r--   2 root hadoop      13899 2020-06-27 10:58 /flightsSFO.parquet/part-00002-b35f850f-3f08-414a-b167-e6d68e7870a3-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /flightsSFO.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comprobamos dónde están los datos de la tabla persistente manejada que habíamos guardado con saveAsTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "-rw-r--r--   2 root hadoop          0 2020-06-27 10:52 /user/hive/warehouse/flightsjfk/_SUCCESS\n",
      "-rw-r--r--   2 root hadoop     177679 2020-06-27 10:52 /user/hive/warehouse/flightsjfk/part-00000-0378514d-d352-4ded-8397-f6b21a3228a2-c000.snappy.parquet\n",
      "-rw-r--r--   2 root hadoop     245568 2020-06-27 10:52 /user/hive/warehouse/flightsjfk/part-00001-0378514d-d352-4ded-8397-f6b21a3228a2-c000.snappy.parquet\n",
      "-rw-r--r--   2 root hadoop      71420 2020-06-27 10:52 /user/hive/warehouse/flightsjfk/part-00002-0378514d-d352-4ded-8397-f6b21a3228a2-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/flightsjfk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Borramos la tabla y comprobamos que, al ser manejada, Spark ha borrado físicamente esos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "|        |weatherdistancetable|       true|\n",
      "+--------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"drop table flightsjfk\")\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: `/user/hive/warehouse/flightsjfk': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/flightsjfk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
